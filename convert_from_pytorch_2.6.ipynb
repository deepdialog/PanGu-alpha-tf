{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf2gpt.model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9G\tPangu-alpha_2.6B_mgt/iter_0001000/mp_rank_00/model_optim_rng.pt\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh Pangu-alpha_2.6B_mgt/iter_0001000/mp_rank_00/model_optim_rng.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = torch.load('Pangu-alpha_2.6B_mgt/iter_0001000/mp_rank_00/model_optim_rng.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_weights = []\n",
    "\n",
    "def extract_weight(w = m0['model'], root=''):\n",
    "    for k, v in w.items():\n",
    "        if isinstance(v, dict):\n",
    "            extract_weight(v, root + '.' + k)\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            k = root + '.' + k\n",
    "            k = k.replace('.language_model.', '')\n",
    "            k = k.replace('.topQueryLayer.', '.layers.31.')\n",
    "            m0_weights.append((\n",
    "                k,\n",
    "                v\n",
    "            ))\n",
    "        else:\n",
    "            print('what?', type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m0_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.word_embeddings.weight torch.Size([40064, 2560])\n",
      "embedding.position_embeddings.weight torch.Size([1024, 2560])\n",
      "topQueryEmbedding.top_query_embeddings.weight torch.Size([1024, 2560])\n",
      "transformer.layers.0.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.0.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.0.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.0.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.0.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.0.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.0.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.0.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.0.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.0.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.0.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.0.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.0.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.0.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.0.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.0.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.1.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.1.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.1.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.1.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.1.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.1.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.1.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.1.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.1.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.1.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.1.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.1.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.1.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.1.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.1.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.1.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.2.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.2.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.2.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.2.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.2.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.2.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.2.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.2.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.2.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.2.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.2.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.2.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.2.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.2.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.2.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.2.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.3.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.3.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.3.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.3.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.3.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.3.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.3.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.3.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.3.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.3.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.3.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.3.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.3.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.3.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.3.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.3.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.4.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.4.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.4.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.4.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.4.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.4.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.4.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.4.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.4.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.4.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.4.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.4.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.4.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.4.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.4.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.4.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.5.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.5.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.5.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.5.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.5.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.5.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.5.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.5.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.5.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.5.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.5.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.5.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.5.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.5.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.5.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.5.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.6.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.6.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.6.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.6.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.6.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.6.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.6.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.6.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.6.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.6.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.6.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.6.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.6.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.6.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.6.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.6.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.7.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.7.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.7.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.7.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.7.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.7.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.7.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.7.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.7.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.7.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.7.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.7.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.7.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.7.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.7.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.7.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.8.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.8.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.8.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.8.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.8.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.8.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.8.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.8.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.8.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.8.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.8.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.8.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.8.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.8.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.8.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.8.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.9.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.9.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.9.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.9.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.9.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.9.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.9.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.9.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.9.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.9.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.9.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.9.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.9.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.9.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.9.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.9.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.10.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.10.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.10.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.10.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.10.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.10.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.10.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.10.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.10.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.10.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.10.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.10.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.10.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.10.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.10.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.10.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.11.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.11.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.11.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.11.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.11.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.11.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.11.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.11.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.11.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.11.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.11.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.11.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.11.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.11.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.11.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.11.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.12.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.12.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.12.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.12.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.12.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.12.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.12.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.12.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.12.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.12.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.12.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.12.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.12.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.12.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.12.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.12.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.13.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.13.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.13.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.13.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.13.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.13.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.13.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.13.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.13.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.13.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.13.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.13.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.13.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.13.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.13.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.13.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.14.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.14.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.14.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.14.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.14.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.14.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.14.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.14.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.14.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.14.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.14.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.14.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.14.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.14.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.14.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.14.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.15.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.15.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.15.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.15.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.15.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.15.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.15.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.15.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.15.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.15.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.15.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.15.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.15.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.15.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.15.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.15.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.16.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.16.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.16.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.16.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.16.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.16.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.16.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.16.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.16.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.16.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.16.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.16.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.16.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.16.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.16.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.16.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.17.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.17.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.17.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.17.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.17.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.17.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.17.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.17.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.17.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.17.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.17.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.17.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.17.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.17.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.17.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.17.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.18.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.18.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.18.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.18.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.18.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.18.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.18.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.18.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.18.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.18.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.18.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.18.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.18.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.18.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.18.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.18.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.19.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.19.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.19.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.19.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.19.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.19.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.19.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.19.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.19.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.19.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.19.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.19.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.19.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.19.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.19.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.19.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.20.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.20.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.20.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.20.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.20.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.20.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.20.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.20.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.20.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.20.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.20.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.20.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.20.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.20.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.20.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.20.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.21.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.21.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.21.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.21.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.21.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.21.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.21.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.21.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.21.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.21.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.21.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.21.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.21.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.21.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.21.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.21.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.22.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.22.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.22.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.22.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.22.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.22.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.22.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.22.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.22.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.22.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.22.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.22.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.22.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.22.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.22.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.22.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.23.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.23.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.23.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.23.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.23.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.23.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.23.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.23.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.23.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.23.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.23.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.23.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.23.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.23.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.23.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.23.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.24.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.24.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.24.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.24.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.24.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.24.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.24.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.24.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.24.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.24.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.24.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.24.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.24.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.24.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.24.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.24.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.25.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.25.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.25.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.25.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.25.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.25.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.25.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.25.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.25.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.25.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.25.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.25.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.25.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.25.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.25.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.25.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.26.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.26.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.26.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.26.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.26.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.26.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.26.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.26.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.26.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.26.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.26.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.26.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.26.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.26.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.26.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.26.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.27.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.27.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.27.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.27.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.27.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.27.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.27.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.27.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.27.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.27.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.27.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.27.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.27.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.27.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.27.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.27.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.28.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.28.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.28.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.28.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.28.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.28.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.28.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.28.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.28.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.28.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.28.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.28.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.28.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.28.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.28.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.28.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.29.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.29.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.29.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.29.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.29.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.29.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.29.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.29.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.29.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.29.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.29.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.29.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.29.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.29.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.29.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.29.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.30.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.30.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.30.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.30.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.30.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.30.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.30.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.30.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.30.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.30.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.30.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.30.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.30.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.30.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.30.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.30.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.layers.31.input_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.31.input_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.31.attention.query.weight torch.Size([2560, 2560])\n",
      "transformer.layers.31.attention.query.bias torch.Size([2560])\n",
      "transformer.layers.31.attention.key.weight torch.Size([2560, 2560])\n",
      "transformer.layers.31.attention.key.bias torch.Size([2560])\n",
      "transformer.layers.31.attention.value.weight torch.Size([2560, 2560])\n",
      "transformer.layers.31.attention.value.bias torch.Size([2560])\n",
      "transformer.layers.31.attention.dense.weight torch.Size([2560, 2560])\n",
      "transformer.layers.31.attention.dense.bias torch.Size([2560])\n",
      "transformer.layers.31.post_attention_layernorm.weight torch.Size([2560])\n",
      "transformer.layers.31.post_attention_layernorm.bias torch.Size([2560])\n",
      "transformer.layers.31.mlp.dense_h_to_4h.weight torch.Size([10240, 2560])\n",
      "transformer.layers.31.mlp.dense_h_to_4h.bias torch.Size([10240])\n",
      "transformer.layers.31.mlp.dense_4h_to_h.weight torch.Size([2560, 10240])\n",
      "transformer.layers.31.mlp.dense_4h_to_h.bias torch.Size([2560])\n",
      "transformer.final_layernorm.weight torch.Size([2560])\n",
      "transformer.final_layernorm.bias torch.Size([2560])\n"
     ]
    }
   ],
   "source": [
    "pangu_weights = {}\n",
    "for k, v in m0_weights:\n",
    "    print(k, v.shape)\n",
    "    pangu_weights[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(\n",
    "    vocab_size=40_064,\n",
    "    layer_size=32,\n",
    "    block_size=1024,\n",
    "    embedding_dropout=0.0,\n",
    "    embedding_size=2560,\n",
    "    num_attention_heads=32,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 40064)\n"
     ]
    }
   ],
   "source": [
    "print(gpt(tf.constant([[1]])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt/embedding/embeddings:0 (40064, 2560)\n",
      "position_embeddings:0 (1024, 2560)\n",
      "top_query:0 (1024, 2560)\n",
      "gpt/layer00/attention/query_layer/kernel:0 (2560, 2560)\n",
      "gpt/layer00/attention/query_layer/bias:0 (2560,)\n",
      "gpt/layer00/attention/key_layer/kernel:0 (2560, 2560)\n",
      "gpt/layer00/attention/key_layer/bias:0 (2560,)\n",
      "gpt/layer00/attention/value_layer/kernel:0 (2560, 2560)\n",
      "gpt/layer00/attention/value_layer/bias:0 (2560,)\n",
      "gpt/layer00/attention/context_projection_layer/kernel:0 (2560, 2560)\n",
      "gpt/layer00/attention/context_projection_layer/bias:0 (2560,)\n",
      "gpt/layer00/LayerNorm_mlp_ln0/gamma:0 (2560,)\n",
      "gpt/layer00/LayerNorm_mlp_ln0/beta:0 (2560,)\n",
      "gpt/layer00/LayerNorm_mlp_ln1/gamma:0 (2560,)\n",
      "gpt/layer00/LayerNorm_mlp_ln1/beta:0 (2560,)\n",
      "gpt/layer00/intermediate/kernel:0 (2560, 10240)\n",
      "gpt/layer00/intermediate/bias:0 (10240,)\n",
      "gpt/layer00/output/kernel:0 (10240, 2560)\n",
      "gpt/layer00/output/bias:0 (2560,)\n",
      "gpt/LayerNorm_final_norm/gamma:0 (2560,)\n",
      "gpt/LayerNorm_final_norm/beta:0 (2560,)\n"
     ]
    }
   ],
   "source": [
    "for x in gpt.weights:\n",
    "    if 'gpt/layer' in x.name:\n",
    "        if 'gpt/layer00' in x.name:\n",
    "            print(x.name, x.shape)\n",
    "    else:\n",
    "        print(x.name, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "\n",
    "for x in gpt.weights:\n",
    "    xs = tuple(x.shape)\n",
    "\n",
    "    if 'gpt/embedding/embeddings:' in x.name:\n",
    "        pname = 'embedding.word_embeddings.weight'\n",
    "        w = pangu_weights[pname]\n",
    "        assert w.shape == (4_0064, 2560)\n",
    "        new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "    elif 'position_embeddings' in x.name:\n",
    "        pname = 'embedding.position_embeddings.weight'\n",
    "        w = pangu_weights[pname]\n",
    "        assert xs == w.shape\n",
    "        new_weights.append((x.name, xs, pname, w))\n",
    "    \n",
    "    elif 'top_query' in x.name:\n",
    "        pname = 'topQueryEmbedding.top_query_embeddings.weight'\n",
    "        w = pangu_weights[pname]\n",
    "        assert xs == w.shape\n",
    "        new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "    elif 'gpt/layer' in x.name:\n",
    "        n_layer = int(x.name[len('gpt/layer'):][:2])\n",
    "        if 'query_layer/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.query.weight'\n",
    "            w = pangu_weights[pname]\n",
    "            w = np.transpose(w)\n",
    "            assert xs == w.shape\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "        elif 'key_layer/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.key.weight'\n",
    "            w = pangu_weights[pname]\n",
    "            w = np.transpose(w)\n",
    "            assert xs == w.shape\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "        elif 'value_layer/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.value.weight'\n",
    "            w = pangu_weights[pname]\n",
    "            w = np.transpose(w)\n",
    "            assert xs == w.shape\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "        elif 'query_layer/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.query.bias'\n",
    "            w = pangu_weights[pname]\n",
    "            assert xs == w.shape\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "        elif 'key_layer/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.key.bias'\n",
    "            w = pangu_weights[pname]\n",
    "            assert xs == w.shape\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "        elif 'value_layer/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.value.bias'\n",
    "            w = pangu_weights[pname]\n",
    "            assert xs == w.shape\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'attention/context_projection_layer/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.dense.weight'\n",
    "            w = pangu_weights[pname]\n",
    "            w = np.transpose(w)\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'attention/context_projection_layer/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.attention.dense.bias'\n",
    "            w = pangu_weights[pname]\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, xs, pname, w))\n",
    "\n",
    "        elif 'LayerNorm_mlp_ln0/gamma' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.input_layernorm.weight'\n",
    "            w = pangu_weights[pname]\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        elif 'LayerNorm_mlp_ln1/gamma' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.post_attention_layernorm.weight'\n",
    "            w = pangu_weights[pname]\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        elif 'LayerNorm_mlp_ln0/beta' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.input_layernorm.bias'\n",
    "            w = pangu_weights[pname]\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        elif 'LayerNorm_mlp_ln1/beta' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.post_attention_layernorm.bias'\n",
    "            w = pangu_weights[pname]\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        elif 'intermediate/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.mlp.dense_h_to_4h.weight'\n",
    "            w = pangu_weights[pname]\n",
    "            w = np.transpose(w)\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        elif 'intermediate/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.mlp.dense_h_to_4h.bias'\n",
    "            w = pangu_weights[pname]\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        elif '/output/kernel' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.mlp.dense_4h_to_h.weight'\n",
    "            w = pangu_weights[pname]\n",
    "            w = np.transpose(w)\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        elif '/output/bias' in x.name:\n",
    "            pname = f'transformer.layers.{n_layer}.mlp.dense_4h_to_h.bias'\n",
    "            w = pangu_weights[pname]\n",
    "            assert w.shape == xs\n",
    "            new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "        else:\n",
    "            print('BAD', x.name, xs)\n",
    "            break\n",
    "    elif 'gpt/LayerNorm_final_norm/gamma' in x.name:\n",
    "        pname = 'transformer.final_layernorm.weight'\n",
    "        w = pangu_weights[pname]\n",
    "        assert w.shape == xs\n",
    "        new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "    elif 'gpt/LayerNorm_final_norm/beta' in x.name:\n",
    "        pname = 'transformer.final_layernorm.bias'\n",
    "        w = pangu_weights[pname]\n",
    "        assert w.shape == xs\n",
    "        new_weights.append((x.name, x.shape, pname, w))\n",
    "\n",
    "    else:\n",
    "        print('BAD', x.name, xs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(new_weights) == len(gpt.weights)\n",
    "for x in new_weights:\n",
    "    assert tuple(x[1]) == x[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.set_weights([x[-1] for x in new_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_jieba import JIEBATokenizer\n",
    "cbpe = JIEBATokenizer(\n",
    "    'PanGu-Alpha-GPU/panguAlpha_pytorch/megatron/tokenizer/bpe_4w_pcl/vocab.vocab',\n",
    "    'PanGu-Alpha-GPU/panguAlpha_pytorch/megatron/tokenizer/bpe_4w_pcl/vocab.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbpe.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 青椒肉丝的做法:是\n",
      "1 青椒肉丝的做法:是青\n",
      "2 青椒肉丝的做法:是青椒\n",
      "3 青椒肉丝的做法:是青椒洗净\n",
      "4 青椒肉丝的做法:是青椒洗净切\n",
      "5 青椒肉丝的做法:是青椒洗净切丝\n",
      "6 青椒肉丝的做法:是青椒洗净切丝<eot>\n",
      "7 青椒肉丝的做法:是青椒洗净切丝<eot>青\n",
      "8 青椒肉丝的做法:是青椒洗净切丝<eot>青椒\n",
      "9 青椒肉丝的做法:是青椒洗净切丝<eot>青椒肉\n"
     ]
    }
   ],
   "source": [
    "ids = cbpe.encode('青椒肉丝的做法：')\n",
    "\n",
    "for i in range(10):\n",
    "    output = gpt(tf.constant([ids]))\n",
    "    nid = np.argmax(output[0, -1])\n",
    "    ids += [int(nid)]\n",
    "    print(i, cbpe.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def batch_gather(a, b):\n",
    "    return tf.gather(a, b, batch_dims=1)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def top_k_top_p_sample(logits, num_samples=1, top_k=0, p=0.95):\n",
    "    batch_size, vocab_size = logits.shape\n",
    "    probs = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # [batch_size, vocab_perm]\n",
    "    indices = tf.argsort(probs, direction='DESCENDING')\n",
    "    logits_to_use = batch_gather(logits, indices)\n",
    "    cumulative_probabilities = tf.math.cumsum(batch_gather(probs, indices), axis=-1, exclusive=False)\n",
    "\n",
    "    # find the top pth index to cut off. careful we don't want to cutoff everything!\n",
    "    # result will be [batch_size, vocab_perm]\n",
    "    if p > 0.0:\n",
    "        exclude_mask = tf.logical_not(\n",
    "            tf.logical_or(cumulative_probabilities < p, tf.range(vocab_size)[None] < 1))\n",
    "        # OPTION A - sample in the sorted space, then unsort.\n",
    "        logits_to_use = logits_to_use - tf.cast(exclude_mask, tf.float32) * 1e10\n",
    "    \n",
    "    if top_k > 0:\n",
    "        logits_to_use = logits_to_use - tf.cast(\n",
    "            tf.argsort(logits_to_use, direction='DESCENDING') >= top_k,\n",
    "            dtype=tf.float32\n",
    "        ) * 1e10\n",
    "    \n",
    "    sample_perm = tf.random.categorical(logits=logits_to_use, num_samples=num_samples)\n",
    "    sample = batch_gather(indices, sample_perm)\n",
    "\n",
    "    return tf.cast(sample, tf.int64)\n",
    "\n",
    "@tf.function\n",
    "def serve(inputs):\n",
    "    return gpt(inputs, kv_cache=None, use_cache=True)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def serve_cache(inputs, kv_cache):\n",
    "    return gpt(inputs, kv_cache=kv_cache, use_cache=True)\n",
    "\n",
    "serve_concrete = serve.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\")\n",
    ")\n",
    "\n",
    "layer_size = 32\n",
    "attention_head = 32\n",
    "embedding_size = 2560\n",
    "\n",
    "serve_cache_concrete = serve_cache.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\"),\n",
    "    tf.TensorSpec(shape=[\n",
    "        layer_size, None, 2, attention_head,\n",
    "        None, embedding_size // attention_head\n",
    "    ], dtype=tf.float32, name=\"kv_cache\")\n",
    ")\n",
    "\n",
    "@tf.function\n",
    "def sample(initial_inputs, length, top_k, top_p, temperature):\n",
    "    layer_size = 32\n",
    "    embedding_size = 2560\n",
    "    attention_head = 32\n",
    "\n",
    "    i = tf.constant(0, dtype=tf.int64)\n",
    "    initial_logits, kv_cache = serve(initial_inputs)\n",
    "    logits_with_temperature = initial_logits[:, -1, :]\n",
    "    if temperature > 0.0:\n",
    "        logits_with_temperature /= temperature\n",
    "    inputs = top_k_top_p_sample(logits_with_temperature, 1, top_k, top_p)\n",
    "    stores = tf.concat([initial_inputs, inputs], axis=1)\n",
    "\n",
    "    def _cond(i, inputs, kv_cache, stores):\n",
    "        return i < length\n",
    "\n",
    "    def _body(i, inputs, kv_cache, stores):\n",
    "        new_logits, new_kv_cache = serve_cache(inputs, kv_cache)\n",
    "        logits_with_temperature = new_logits[:, -1, :]\n",
    "        if temperature > 0.0:\n",
    "            logits_with_temperature /= temperature\n",
    "        new_inputs = top_k_top_p_sample(logits_with_temperature, 1, top_k, top_p)\n",
    "        new_stores = tf.concat([stores, new_inputs], axis=-1)\n",
    "        new_kv_cache = tf.concat([\n",
    "            kv_cache,\n",
    "            new_kv_cache\n",
    "        ], axis=-2)\n",
    "        new_i = i + 1\n",
    "        return [new_i, new_inputs, new_kv_cache, new_stores]\n",
    "\n",
    "    result = tf.while_loop(\n",
    "        _cond, _body,\n",
    "        loop_vars=[i, inputs, kv_cache, stores],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape(None),\n",
    "            tf.TensorShape([None, None]),\n",
    "            tf.TensorShape([\n",
    "                layer_size, None, 2,\n",
    "                attention_head, None,\n",
    "                embedding_size // attention_head\n",
    "            ]),\n",
    "            tf.TensorShape([\n",
    "                None, None\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "    return result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  465   235   464  1123    10    21    18 32636  3001 21507    13     3\n",
      "   3001  3001 32504    10  2448    24    58   201]], shape=(1, 20), dtype=int64)\n",
      "今天天气不错,我在树下吹吹风\n",
      "吹吹凉风,心情也会跟\n"
     ]
    }
   ],
   "source": [
    "ids = cbpe.encode('今天天气不错')\n",
    "\n",
    "ret = sample(\n",
    "    tf.constant([ids], dtype=tf.int64),\n",
    "    tf.constant(15, dtype=tf.int64),\n",
    "    tf.constant(15, dtype=tf.int32),\n",
    "    tf.constant(0.95, dtype=tf.float32),\n",
    "    tf.constant(0.9, dtype=tf.float32)\n",
    ")\n",
    "print(ret)\n",
    "print(cbpe.decode(ret.numpy().tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399c8d100>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd1218df6a0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399baf9a0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399ae0c70>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399aedf40>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a81280>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a8d550>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a9c7f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399aaaaf0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399ab5dc0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a42fa0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a553a0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a64670>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a70940>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a7fc10>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a0cee0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a201f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a2f4c0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399a3d790>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd3999caa60>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd3999d8d30>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd3999e5f10>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd3999fa310>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399987610>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd3999948e0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd3999a2bb0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd3999afe80>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399945190>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399952460>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd399961730>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd39996fa00>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x7fd39997ccd0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as attention_layer_call_fn, attention_layer_call_and_return_conditional_losses, LayerNorm_mlp_ln0_layer_call_fn, LayerNorm_mlp_ln0_layer_call_and_return_conditional_losses, LayerNorm_mlp_ln1_layer_call_fn while saving (showing 5 of 1920). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./pangu-2.6B-tf2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./pangu-2.6B-tf2/assets\n"
     ]
    }
   ],
   "source": [
    "gpt.save('./pangu-2.6B-tf2', include_optimizer=False, signatures={\n",
    "    'serving_default': sample.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64, name=\"inp\"),\n",
    "        tf.TensorSpec(shape=[None,], dtype=tf.int64, name=\"length\"),\n",
    "        tf.TensorSpec(shape=[None,], dtype=tf.int32, name=\"top_k\"),\n",
    "        tf.TensorSpec(shape=[None,], dtype=tf.float32, name=\"top_p\"),\n",
    "        tf.TensorSpec(shape=[None,], dtype=tf.float32, name=\"temperature\")\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
